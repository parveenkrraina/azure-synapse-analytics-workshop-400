{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Delta Lake features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Use `spark.read.csv()` to load the data from the source public blob storage account and display its schema and shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "manualSchema = StructType([\n",
        "  StructField(\"CustomerId\", StringType(), True),\n",
        "  StructField(\"ProductId\", StringType(), True),\n",
        "  StructField(\"Rating\", LongType(), True),\n",
        "  StructField(\"Cost\", FloatType(), True),\n",
        "  StructField(\"Size\", FloatType(), True),\n",
        "  StructField(\"Price\", FloatType(), True),\n",
        "  StructField(\"PrimaryBrandId\", LongType(), True),\n",
        "  StructField(\"GenderId\", LongType(), True),\n",
        "  StructField(\"MaritalStatus\", LongType(), True),\n",
        "  StructField(\"LowerIncomeBound\", FloatType(), True),\n",
        "  StructField(\"UpperIncomeBound\", FloatType(), True)\n",
        "])\n",
        "\n",
        "url = \"wasbs://files@synapsemlpublic.blob.core.windows.net/PersonalizedData.csv\"\n",
        "raw_data = spark.read.csv(url, header=True, schema=manualSchema)\n",
        "print(\"Schema: \")\n",
        "raw_data.printSchema()\n",
        "\n",
        "df = raw_data.toPandas()\n",
        "print(\"Shape: \", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Save the customer rating dataframe as a Delta Lake table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "delta_table_path = 'abfss://delta@#DATA_LAKE_ACCOUNT_NAME#.dfs.core.windows.net/customer-rating'\n",
        "\n",
        "raw_data.write.format('delta').save(delta_table_path)\n",
        "\n",
        "mssparkutils.fs.ls(delta_table_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Explore the layout of files and inspect the Delta log file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "delta_log_path = mssparkutils.fs.ls(f'{delta_table_path}/_delta_log')[0].path\n",
        "print(delta_log_path)\n",
        "mssparkutils.fs.head(delta_log_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Load the Delta lake table into a Spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "data = spark.read.format('delta').load(delta_table_path)\n",
        "data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Use the dedicated `DeltaTable` class to manage the Delta Lake table. Explore the Delta Lake table history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get all versions\n",
        "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
        "display(delta_table.history())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Perform an update on the Delta Lake table using a SQL-style condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Declare the predicate by using a SQL-formatted string.\n",
        "delta_table.update(\n",
        "  condition = \"Price < 1500\",\n",
        "  set = { \"Price\": \"Price * 1.05\" }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Check again the history of the Delta Lake table and notice the new entry corresponding to the update that has just been performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "display(delta_table.history())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "It's possible to query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "display(spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(delta_table_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "display(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(delta_table_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Create the metadata to expose the Delta Lake table in the default Spark database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "spark.sql(\"CREATE TABLE CustomerRating USING DELTA LOCATION '{0}'\".format(delta_table_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "List all tables that exist in the default Spark database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "spark.sql(\"SHOW TABLES\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Explore the properties of the `CustomerRating` Spark database table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "spark.sql(\"DESCRIBE EXTENDED customerrating\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "To query the delta table from the serverless SQL pool, navigate to the `Develop` hub in Synapse Studio and create a new SQL script. Make sure `Built-in` is selected for the `Connect to` option and `default` is selected for the `Use database` option.\n",
        "\n",
        "Enter the query below and make sure you replace `<your_data_lake_account_name>` with the name of the Data Lake account with the one from your lab environment.\n",
        "\n",
        "\n",
        "```sql\n",
        "SELECT TOP 10 *\n",
        "FROM OPENROWSET(\n",
        "    BULK 'abfss://delta@<your_data_lake_account_name>.dfs.core.windows.net/customer-rating/',\n",
        "    FORMAT = 'delta') as rows\n",
        "```\n",
        "\n",
        "![Query Delta Lake with serverless SQL pool](https://solliancepublicdata.blob.core.windows.net/synapse-l400/notebook-images/query-delta-table.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "This concludes the Delta Lake section of this notebook.\n",
        "\n",
        "To learn more about Delta Lake support in Syanspe Spark, take a look at the [Work with Delta Lake](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-delta-lake-overview?pivots=programming-language-python) section in the Azure Synapse Analytics documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Mount storage account containers\n",
        "\n",
        "The `mssparkutils` utility can be used to mount storage account containers. In the example below, you will use an already created linked service to manage the authentication with the storage account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-08T12:45:22.0296104Z",
              "execution_start_time": "2022-02-08T12:44:59.1814619Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-08T12:44:59.0495819Z",
              "session_id": 7,
              "session_start_time": null,
              "spark_pool": "SparkPool02",
              "state": "finished",
              "statement_id": 6
            },
            "text/plain": [
              "StatementMeta(SparkPool02, 7, 6, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "mssparkutils.fs.mount( \n",
        "    \"abfss://delta@#DATA_LAKE_ACCOUNT_NAME#.dfs.core.windows.net\", \n",
        "    \"/test\", \n",
        "    {\"linkedService\":\"#DATA_LAKE_ACCOUNT_NAME#\"} \n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Explore the content of the mounted volume using the local path. Note the `synfs:/{jobId}` prefix used by `mssparkutils` for the local path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-08T12:45:31.5152351Z",
              "execution_start_time": "2022-02-08T12:45:31.3644754Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-08T12:45:31.274196Z",
              "session_id": 7,
              "session_start_time": null,
              "spark_pool": "SparkPool02",
              "state": "finished",
              "statement_id": 7
            },
            "text/plain": [
              "StatementMeta(SparkPool02, 7, 7, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[FileInfo(path=synfs:/7/test/customer-rating/_delta_log/00000000000000000000.json, name=00000000000000000000.json, size=2299),\n",
              " FileInfo(path=synfs:/7/test/customer-rating/_delta_log/00000000000000000001.json, name=00000000000000000001.json, size=1466)]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "jobId = mssparkutils.env.getJobId() \n",
        "\n",
        "log_files = mssparkutils.fs.ls(f'synfs:/{jobId}/test/customer-rating/_delta_log')\n",
        "log_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "When using regular PySpark classes, the syntax of the prefix is slightly different - `/synfs/{jobId}`. Use this prefix to load and display the first 500 characters from the first Delta Lake log file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-02-08T12:45:33.9521758Z",
              "execution_start_time": "2022-02-08T12:45:33.4552981Z",
              "livy_statement_state": "available",
              "queued_time": "2022-02-08T12:45:33.3618394Z",
              "session_id": 7,
              "session_start_time": null,
              "spark_pool": "SparkPool02",
              "state": "finished",
              "statement_id": 8
            },
            "text/plain": [
              "StatementMeta(SparkPool02, 7, 8, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'{\"commitInfo\":{\"timestamp\":1644308193124,\"operation\":\"WRITE\",\"operationParameters\":{\"mode\":\"ErrorIfExists\",\"partitionBy\":\"[]\"},\"isBlindAppend\":true,\"operationMetrics\":{\"numFiles\":\"1\",\"numOutputBytes\":\"135572\",\"numOutputRows\":\"5000\"}}}\\n{\"protocol\":{\"minReaderVersion\":1,\"minWriterVersion\":2}}\\n{\"metaData\":{\"id\":\"ae14ef8d-1182-44af-9a85-76f991697c24\",\"format\":{\"provider\":\"parquet\",\"options\":{}},\"schemaString\":\"{\\\\\"type\\\\\":\\\\\"struct\\\\\",\\\\\"fields\\\\\":[{\\\\\"name\\\\\":\\\\\"CustomerId\\\\\",\\\\\"type\\\\\":\\\\\"string\\\\\",\\\\\"nullable\\\\\"'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with open(f'/synfs/{jobId}/test/customer-rating/_delta_log/{log_files[0].name}', 'r') as f:\n",
        "    f.read()[:500]"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
